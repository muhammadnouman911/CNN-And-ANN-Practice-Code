{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "176rzTChMgF3NUfOFBgPSEmdE8b-pDwy1",
      "authorship_tag": "ABX9TyOhxf3bUdJCVuK1mJzcLUzA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhammadnouman911/CNN-And-ANN-Practice-Code/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss8_ErQ-8Lfm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If using Google Colab, mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set path to dataset\n",
        "train_path = '/content/drive/MyDrive/CNN/dataset/training_set'\n",
        "test_path = '/content/drive/MyDrive/CNN/dataset/test_set'\n",
        "predict_path = '/content/drive/MyDrive/CNN/dataset/single_prediction'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tXBgYjg9PDH",
        "outputId": "7c90de47-95df-4db6-901c-b86f92b56f87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing the Training set\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory(train_path,\n",
        "                                                 target_size=(64, 64),\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='binary')\n",
        "\n",
        "# Preprocessing the Test set\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(test_path,\n",
        "                                            target_size=(64, 64),\n",
        "                                            batch_size=32,\n",
        "                                            class_mode='binary')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjMPQKeX9U3O",
        "outputId": "879cbf7f-66aa-4fa0-cdea-4eefca6f26c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8000 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = tf.keras.models.Sequential()\n",
        "\n",
        "# Convolution\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n",
        "\n",
        "# Pooling\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "# Second convolutional layer\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "# Flattening\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "\n",
        "# Fully connected layer\n",
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
        "\n",
        "# Output layer\n",
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "hpobIPA29Vh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "t-X6S87N9X3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.fit(x=training_set, validation_data=test_set, epochs=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-2LeDo99fBN",
        "outputId": "f4ca83c6-6908-4068-9a24-befc113c4f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m625s\u001b[0m 3s/step - accuracy: 0.5481 - loss: 0.6848 - val_accuracy: 0.7170 - val_loss: 0.5693\n",
            "Epoch 2/5\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 368ms/step - accuracy: 0.6813 - loss: 0.5930 - val_accuracy: 0.7190 - val_loss: 0.5696\n",
            "Epoch 3/5\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 365ms/step - accuracy: 0.7289 - loss: 0.5429 - val_accuracy: 0.7555 - val_loss: 0.5126\n",
            "Epoch 4/5\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 363ms/step - accuracy: 0.7528 - loss: 0.5099 - val_accuracy: 0.7625 - val_loss: 0.4936\n",
            "Epoch 5/5\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 365ms/step - accuracy: 0.7636 - loss: 0.4845 - val_accuracy: 0.7720 - val_loss: 0.4767\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7baf637de850>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for img_file in os.listdir(predict_path):\n",
        "    img_path = os.path.join(predict_path, img_file)\n",
        "    test_image = image.load_img(img_path, target_size=(64, 64))\n",
        "    test_image = image.img_to_array(test_image)\n",
        "    test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "    result = cnn.predict(test_image)\n",
        "    training_set.class_indices\n",
        "\n",
        "    if result[0][0] > 0.5:\n",
        "        prediction = 'dog'\n",
        "    else:\n",
        "        prediction = 'cat'\n",
        "\n",
        "    print(f\"{img_file}: {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zitgUYuq9iWe",
        "outputId": "11f32511-e580-43f9-9e71-4c26cde38232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
            "cat_or_dog_1.jpg: dog\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "cat_or_dog_2.jpg: cat\n"
          ]
        }
      ]
    }
  ]
}